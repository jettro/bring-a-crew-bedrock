{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"eu-west-1\")\n",
    "\n",
    "\n",
    "def print_results_in_html(bedrock_response):\n",
    "    html_content = \"<html><body>\"\n",
    "    html_content += \"<h3>Response</h3>\"\n",
    "    html_content += \"<pre>\"\n",
    "    for item in bedrock_response:\n",
    "        if \"text\" in item:\n",
    "            html_content += item[\"text\"]\n",
    "        elif \"toolUse\" in item:\n",
    "            html_content += f\"Tool Use: {item['toolUse']}\"\n",
    "        elif \"toolResult\" in item:\n",
    "            html_content += f\"Tool Result: {item['toolResult']}\"\n",
    "        else:\n",
    "            html_content += str(item)\n",
    "    html_content += \"</pre>\"\n",
    "    html_content += \"</body></html>\"\n",
    "\n",
    "    display(HTML(html_content))\n"
   ],
   "id": "8b219e4acf3ebc68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Talk to an LLM\n",
    "LLMs, or Large Language Models, became famous through ChatGPT. A lot of people feel comfortable using chat to interact with an LLM. The following code block shows how to ask the LLM a question."
   ],
   "id": "ab8e8a59a6f0af8"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "model_id = \"eu.amazon.nova-lite-v1:0\"\n",
    "\n",
    "def ask_question(question: str) -> str:\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": question}],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # Send the message to the model, using a basic inference configuration.\n",
    "        response = client.converse(\n",
    "            modelId=model_id,\n",
    "            messages=conversation,\n",
    "            inferenceConfig={\"maxTokens\": 512, \"temperature\": 0, \"topP\": 0.9},\n",
    "        )\n",
    "\n",
    "        # Extract and print the response text.\n",
    "        return response[\"output\"][\"message\"][\"content\"]\n",
    "\n",
    "    except (ClientError, Exception) as e:\n",
    "        print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "        exit(1)\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print_results_in_html(ask_question(\"When is Jettro available?\"))",
   "id": "b3eeeba8203f2ca2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Be more specific when formulating your question\n",
    "Ask a better question by articulating what you want. Tell the LLM your intentions."
   ],
   "id": "6561cb20a09f14de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print_results_in_html(ask_question(\"I need to make an appointment with Jettro. When is he available?\"))",
   "id": "df1fa820aa4b3a33",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Provide a system prompt with some guidelines\n",
    "With a system message, you tell the LLM more about the role"
   ],
   "id": "db9d8813ec6cc338"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def ask_question_with_system_prompt(question) -> str:\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": question}],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # Send the message to the model, using a basic inference configuration.\n",
    "        response = client.converse(\n",
    "            modelId=model_id,\n",
    "            messages=conversation,\n",
    "            system=[\n",
    "                {\n",
    "                    \"text\": (\n",
    "                        \"You are a scheduling assistant. You help in checking the availability of people.\"\n",
    "                        \"Do not make up availability if you do not know the person's schedule.\"\n",
    "                        \"Answer in short sentences, stick to the answer to the question.\"\n",
    "                    )\n",
    "                },\n",
    "            ],\n",
    "            inferenceConfig={\"maxTokens\": 512, \"temperature\": 0, \"topP\": 0.9},\n",
    "        )\n",
    "\n",
    "        # Extract and print the response text.\n",
    "        return response[\"output\"][\"message\"][\"content\"]\n",
    "\n",
    "    except (ClientError, Exception) as e:\n",
    "        print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "        exit(1)"
   ],
   "id": "e3714333127ecb60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print_results_in_html(ask_question_with_system_prompt(\"When is Jettro available?\"))",
   "id": "db191f96205016bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Provide a context with the required information\n",
    "The way you obtain the context is not important for the LLM."
   ],
   "id": "b09c3b65ac77f418"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "context = (\n",
    "    \"This is the agenda of people in our office:\\n\"\n",
    "    \"- Jettro is available on Monday and Thursday;\\n\"\n",
    "    \"- Joey is available on Tuesday, Thursday, and Friday;\\n\"\n",
    "    \"- Daniel is available from Monday to Thursday.\"\n",
    ")\n",
    "\n",
    "print_results_in_html(ask_question_with_system_prompt(f\"{context}\\nWhen is Jettro available?\"))"
   ],
   "id": "e1f7fc5f05418ac7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print_results_in_html(ask_question_with_system_prompt(f\"When are Jettro, Joey and Daniel available together?\"))",
   "id": "23fd8349bf07e52e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Give the LLM Memory\n",
    "By keeping the messages in a memory, the LLM can keep using the context."
   ],
   "id": "fcb6fb777081fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def ask_question_with_memory(question, messages=None) -> (str, list[dict]):\n",
    "    conversation = messages or []\n",
    "    conversation.append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": question}],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Send the message to the model, using a basic inference configuration.\n",
    "        response = client.converse(\n",
    "            modelId=model_id,\n",
    "            messages=conversation,\n",
    "            system=[\n",
    "                {\n",
    "                    \"text\": (\n",
    "                        \"You are a scheduling assistant. You help in checking the availability of people.\"\n",
    "                        \"Do not make up availability if you do not know the person's schedule.\"\n",
    "                        \"Answer in short sentences, stick to the answer to the question.\"\n",
    "                    )\n",
    "                },\n",
    "            ],\n",
    "            inferenceConfig={\"maxTokens\": 512, \"temperature\": 0.5, \"topP\": 0.9},\n",
    "        )\n",
    "\n",
    "        # Extract and print the response text.\n",
    "        response_text = response[\"output\"][\"message\"][\"content\"]\n",
    "        conversation.append(response[\"output\"][\"message\"])\n",
    "        return response_text, conversation\n",
    "\n",
    "    except (ClientError, Exception) as e:\n",
    "        print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "\n",
    "conversational_memory = None"
   ],
   "id": "fd2c6faaba2b2883",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "answer, conversational_memory = ask_question_with_memory(question=f\"{context}\\nWhen is Jettro available?\",\n",
    "                                                         messages=conversational_memory)\n",
    "print_results_in_html(answer)"
   ],
   "id": "96a287887f72edc8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "answer, conversational_memory = ask_question_with_memory(question=f\"When are Jettro and Joey both available?\",\n",
    "                                                         messages=conversational_memory)\n",
    "print_results_in_html(answer)"
   ],
   "id": "36065d810ccbfa6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Give the LLM a Tool to ask schedule information\n",
    "By using a Tool, we can have a dynamic context. The Tool is a function that the LLM knows how to call."
   ],
   "id": "d6a48f5f3d2ee403"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# This is the tool\n",
    "def find_person_availability(name: str) -> str:\n",
    "    print(f\"Finding availability for {name}\")\n",
    "    if name.lower() == \"jettro\":\n",
    "        return \"Jettro is available on Tuesday and Thursday.\"\n",
    "    elif name.lower() == \"daniel\":\n",
    "        return \"Daniel is available on Monday to Thursday.\"\n",
    "    elif name.lower() == \"joey\":\n",
    "        return \"Joey is available on Thursday and Friday.\"\n",
    "    else:\n",
    "        return \"I do not know the availability of this person.\"\n"
   ],
   "id": "f3b44744665097",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tool_config = {\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"find_person_availability\",\n",
    "                \"description\": \"Find the availability of a person\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"name\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The name of the person to find availability for.\",\n",
    "                            },\n",
    "                        },\n",
    "                        \"required\": [\"name\"],\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}"
   ],
   "id": "fd178ad878cc2b2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def ask_question_with_tool(question, messages=None) -> (str | list[dict], list[dict]):\n",
    "    conversation = messages or []\n",
    "    # Add the system message to the conversation\n",
    "    if question:\n",
    "        conversation.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"text\": question}],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        # Send the message to the model, using a basic inference configuration.\n",
    "        response = client.converse(\n",
    "            modelId=model_id,\n",
    "            messages=conversation,\n",
    "            system=[\n",
    "                {\n",
    "                    \"text\": (\n",
    "                        \"You are a scheduling assistant. You help in checking the availability of people.\"\n",
    "                        \"Do not make up availability if you do not know the person's schedule.\"\n",
    "                        \"Answer in short sentences, stick to the answer to the question.\"\n",
    "                    )\n",
    "                },\n",
    "            ],\n",
    "            inferenceConfig={\"maxTokens\": 512, \"temperature\": 0.5, \"topP\": 0.9},\n",
    "            toolConfig=tool_config,\n",
    "        )\n",
    "        conversation.append(response[\"output\"][\"message\"])\n",
    "\n",
    "        # Check if the response contains a tool call\n",
    "        stop_reason = response[\"stopReason\"]\n",
    "        if stop_reason == \"tool_use\":\n",
    "            # Extract the tool call information\n",
    "            tool_calls = []\n",
    "            for item in response[\"output\"][\"message\"][\"content\"]:\n",
    "                if \"toolUse\" in item:\n",
    "                    tool_calls.append(item[\"toolUse\"])\n",
    "            return tool_calls, conversation\n",
    "        else:\n",
    "            # Extract and print the response text.\n",
    "            response_text = response[\"output\"][\"message\"][\"content\"]\n",
    "            return response_text, conversation\n",
    "\n",
    "    except (ClientError, Exception) as e:\n",
    "        print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "\n",
    "def add_tool_call_to_memory(tool_calls, messages):\n",
    "    tool_results = []\n",
    "    for tool_call in tool_calls:\n",
    "        if tool_call[\"name\"] == \"find_person_availability\":\n",
    "            name = tool_call[\"input\"][\"name\"]\n",
    "            availability = find_person_availability(name)\n",
    "            tool_result = {\n",
    "                \"toolResult\": {\n",
    "                    \"toolUseId\": tool_call[\"toolUseId\"],\n",
    "                    \"content\": [{\"text\": availability}],\n",
    "                }\n",
    "            }\n",
    "            tool_results.append(tool_result)\n",
    "\n",
    "    messages.append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": tool_results,\n",
    "        }\n",
    "    )\n"
   ],
   "id": "2e3767d7c88408b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "conversational_memory = None\n",
    "\n",
    "answer, conversational_memory = ask_question_with_tool(question=f\"When are Jettro and Joey both available?\",\n",
    "                                                       messages=conversational_memory)\n",
    "print_results_in_html(answer)\n"
   ],
   "id": "771be6d0e3b50829",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "add_tool_call_to_memory(answer, conversational_memory)\n",
    "\n",
    "answer, conversational_memory = ask_question_with_tool(question=None,\n",
    "                                                       messages=conversational_memory)\n",
    "\n",
    "print_results_in_html(answer)"
   ],
   "id": "a027738d021d598f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "adfc85fa7f001974",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
